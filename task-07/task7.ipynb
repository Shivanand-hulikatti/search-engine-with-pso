{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317c9577-b9b9-45ac-bfd5-c9b8feb1fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessing helpers\n",
    "# -----------------------------\n",
    "TOKEN_RE = re.compile(r\"[a-zA-Z0-9]+\")\n",
    "STOPWORDS = set(\n",
    "    \"a about above after again against all am an and any are aren't as at be because been before \"\n",
    "    \"being below between both but by can't cannot could couldn't did didn't do does doesn't doing \"\n",
    "    \"don't down during each few for from further had hadn't has hasn't have haven't having he he'd \"\n",
    "    \"he'll he's her here here's hers herself him himself his how how's i i'd i'll i'm i've if in into \"\n",
    "    \"isn't it it's its itself let's me more most mustn't my myself no nor not of off on once only or \"\n",
    "    \"other ought our ours ourselves out over own same shan't she she'd she'll she's should shouldn't \"\n",
    "    \"so some such than that that's the their theirs them themselves then there there's these they they'd \"\n",
    "    \"they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're \"\n",
    "    \"we've were weren't what what's when when's where where's which while who who's whom why why's with \"\n",
    "    \"won't would wouldn't you you'd you'll you're you've your yours yourself yourselves\".split()\n",
    ")\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[_\\-/\\\\]\", \" \", text)\n",
    "    tokens = TOKEN_RE.findall(text)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def tokenize(text: str, remove_stopwords: bool = True) -> List[str]:\n",
    "    tokens = normalize_text(text).split()\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "# -----------------------------\n",
    "# Parser\n",
    "# -----------------------------\n",
    "class Parser:\n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "        self.documents: List[Dict[str, Any]] = []\n",
    "\n",
    "    def load(self) -> List[Dict[str, Any]]:\n",
    "        with open(self.filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.documents = json.load(f)\n",
    "        return self.documents\n",
    "\n",
    "    def parse(self) -> List[Dict[str, Any]]:\n",
    "        if not self.documents:\n",
    "            self.load()\n",
    "        parsed_docs = []\n",
    "        for i, item in enumerate(self.documents):\n",
    "            attributes = \" \".join(f\"{k} {v}\" for k, v in item.items() if k != \"id\")\n",
    "            combined_text = attributes.strip()\n",
    "            normalized = normalize_text(combined_text)\n",
    "            tokens = tokenize(combined_text)\n",
    "            brand = item.get(\"Brand\", \"\")\n",
    "            ptype = item.get(\"Type\", \"\")\n",
    "            pseudo_title = f\"{brand} {ptype}\".strip()\n",
    "            if not pseudo_title:\n",
    "                pseudo_title = list(item.values())[0]\n",
    "            parsed_docs.append({\n",
    "                \"id\": str(item.get(\"id\", i)),\n",
    "                \"text\": normalized,\n",
    "                \"tokens\": tokens,\n",
    "                \"metadata\": item,\n",
    "                \"title\": pseudo_title\n",
    "            })\n",
    "        return parsed_docs\n",
    "\n",
    "# -----------------------------\n",
    "# PSO Reranker\n",
    "# -----------------------------\n",
    "class PSOReranker:\n",
    "    def __init__(self, docs: List[Dict[str, Any]], top_k: int = 15):\n",
    "        self.docs = docs\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def compute_features(self, doc, query_tokens):\n",
    "        bm25_score = doc.get(\"bm25_score\", 0)\n",
    "        attr_match = len(set(query_tokens) & set(doc[\"tokens\"]))\n",
    "        length_factor = 1 / np.log(1 + len(doc[\"tokens\"]))\n",
    "        return np.array([bm25_score, attr_match, length_factor])\n",
    "\n",
    "    def fitness(self, weights, candidates, query_tokens):\n",
    "        total_score = 0\n",
    "        for doc in candidates:\n",
    "            features = self.compute_features(doc, query_tokens)\n",
    "            total_score += np.dot(weights, features)\n",
    "        return total_score / len(candidates)\n",
    "\n",
    "    def optimize(self, candidates, query, num_particles=20, max_iter=50):\n",
    "        dim = 3\n",
    "        query_tokens = tokenize(query)\n",
    "        pos = np.random.uniform(0.1, 1.0, (num_particles, dim))\n",
    "        vel = np.random.uniform(-0.1, 0.1, (num_particles, dim))\n",
    "        pbest = pos.copy()\n",
    "        pbest_scores = np.array([self.fitness(p, candidates, query_tokens) for p in pos])\n",
    "        gbest_idx = np.argmax(pbest_scores)\n",
    "        gbest = pbest[gbest_idx].copy()\n",
    "        gbest_score = pbest_scores[gbest_idx]\n",
    "\n",
    "        w = 0.5\n",
    "        c1, c2 = 1.0, 2.0\n",
    "        history = []\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            for i in range(num_particles):\n",
    "                score = self.fitness(pos[i], candidates, query_tokens)\n",
    "                if score > pbest_scores[i]:\n",
    "                    pbest[i] = pos[i].copy()\n",
    "                    pbest_scores[i] = score\n",
    "\n",
    "            gbest_idx = np.argmax(pbest_scores)\n",
    "            if pbest_scores[gbest_idx] > gbest_score:\n",
    "                gbest = pbest[gbest_idx].copy()\n",
    "                gbest_score = pbest_scores[gbest_idx]\n",
    "\n",
    "            r1, r2 = np.random.rand(), np.random.rand()\n",
    "            vel = w * vel + c1 * r1 * (pbest - pos) + c2 * r2 * (gbest - pos)\n",
    "            pos = pos + vel\n",
    "            history.append(gbest_score)\n",
    "\n",
    "        return gbest, history\n",
    "\n",
    "# -----------------------------\n",
    "# Unified Search Engine\n",
    "# -----------------------------\n",
    "class SearchEngine:\n",
    "    def __init__(self, docs: List[Dict[str, Any]]):\n",
    "        self.docs = docs\n",
    "        self.tokenized_corpus = [doc[\"tokens\"] for doc in docs]\n",
    "        self.bm25_engine = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 15):\n",
    "        query_tokens = tokenize(query)\n",
    "        scores = self.bm25_engine.get_scores(query_tokens)\n",
    "        candidates = []\n",
    "        for doc, score in zip(self.docs, scores):\n",
    "            doc_copy = doc.copy()\n",
    "            doc_copy[\"bm25_score\"] = score\n",
    "            candidates.append(doc_copy)\n",
    "    \n",
    "        # take top 2*top_k for reranking\n",
    "        candidates = sorted(candidates, key=lambda d: d[\"bm25_score\"], reverse=True)[:2*top_k]\n",
    "    \n",
    "        # PSO reranking\n",
    "        pso = PSOReranker(candidates, top_k=top_k)\n",
    "        best_weights, history = pso.optimize(candidates, query)\n",
    "        for doc in candidates:\n",
    "            features = pso.compute_features(doc, query_tokens)\n",
    "            doc[\"final_score\"] = np.dot(best_weights, features)\n",
    "    \n",
    "        # Reranked top_k results with original JSON + rank + score\n",
    "        reranked = sorted(candidates, key=lambda d: d[\"final_score\"], reverse=True)[:top_k]\n",
    "        results = []\n",
    "        for rank, doc in enumerate(reranked, 1):\n",
    "            result = doc[\"metadata\"].copy()  # original JSON fields\n",
    "            result[\"rank\"] = rank\n",
    "            result[\"score\"] = float(doc[\"final_score\"])\n",
    "            results.append(result)\n",
    "    \n",
    "        return results, best_weights, history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000da6da-1585-4428-a827-0ec47146827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 search results for query: \"prakash\"\n",
      "\n",
      "Rank 1 | ID=2019 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2019\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2 | ID=2039 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2039\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3 | ID=2085 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2085\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4 | ID=2128 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2128\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5 | ID=2161 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2161\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 6 | ID=2201 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2201\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 7 | ID=2240 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2240\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 8 | ID=2280 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2280\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 9 | ID=2328 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2328\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 10 | ID=2355 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2355\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 11 | ID=2391 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2391\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 12 | ID=2435 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2435\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 13 | ID=2480 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2480\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 14 | ID=2510 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2510\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 15 | ID=2554 | Score=49.8134\n",
      "   Imprint: Prakash Books\n",
      "   id: 2554\n",
      "--------------------------------------------------------------------------------\n",
      "Optimized PSO weights: [4.02876152 0.9209867  2.10357702]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Example Usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = Parser(\"data-set.json\")\n",
    "    docs = parser.parse()\n",
    "\n",
    "    engine = SearchEngine(docs)\n",
    "    query = \"prakash\"\n",
    "    results, weights, history = engine.search(query, top_k=15)\n",
    "\n",
    "    print(f\"Top 15 search results for query: \\\"{query}\\\"\\n\")\n",
    "    for r in results:\n",
    "        print(f\"Rank {r['rank']} | ID={r.get('id','N/A')} | Score={r['score']:.4f}\")\n",
    "        # Print key details neatly\n",
    "        for k, v in r.items():\n",
    "            if k not in [\"rank\", \"score\"]:  # exclude added fields\n",
    "                print(f\"   {k}: {v}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    print(\"Optimized PSO weights:\", weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
